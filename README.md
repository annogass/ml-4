მოდელი 1 — simple_cnn_v1 

პირველი მოდელი წარმოადგენს მარტივ კონვოლუციურ ნეირონულ ქსელს (CNN), სახელად SimpleCNN, რომელიც გამოიყენება FER (Facial Expression Recognition) ამოცანის გადასაჭრელად. არქიტექტურა შედგება ორი Convolutional ფენისგან, MaxPooling ფენებისგან და ორი Fully Connected ფენისგან.

არქიტექტურა:
Conv2D (1→32), kernel=3, padding=1
ReLU
MaxPooling2D (2×2)
Conv2D (32→64), kernel=3, padding=1
ReLU
MaxPooling2D (2×2)
FC1: 64×12×12 → 128
Dropout(0.5)
FC2: 128 → 7 (ემოციის კლასები)

ჰიპერპარამეტრები:
ოპტიმიზატორი: Adam
Learning rate: 0.001
Batch size: 64
Epoch-ები: 15

შედეგები:
საბოლოო Train Accuracy: 58.0%
საბოლოო Validation Accuracy: 53.1%
Train Loss (epoch 15): 1.0813
Val Loss (epoch 15): 1.2441

ანალიზი:
მოდელმა აჩვენა პროგრესული გაუმჯობესება სწავლის პროცესში — ყოველ apoch-ზე ტრენინგისა და ვალიდაციის სიზუსტე თანდათან იზრდებოდა. თუმცა, საბოლოო შედეგებიდან ჩანს, რომ მოდელს დაძლეული აქვს ნაწილობრივი overfitting: ტრენინგის სიზუსტე უფრო მაღალია.




მოდელი 2 – SimpleCNN-v2_Deeper_SGD.ipynb
ამ მოდელში შევიტანე მნიშვნელოვანი არქიტექტურული და ჰიპერპარამეტრების ცვლილებები SimpleCNN-ის პირველ ვერსიასთან შედარებით.

არქიტექტურა:
SimpleCNN-v2_Deeper_SGD.ipynb დაფუძნებულია უფრო ღრმა კონვოლუციურ ნეირონულ ქსელზე. მთავარი ცვლილებაა მესამე Conv ფენის დამატება, რაც ქსელს აძლევს მეტ შესაძლებლობას ამოიცნოს სიღრმისეული და კომპლექსური ფუნქციები.

სტრუქტურა:
Conv2D (1→32), kernel=3, padding=1 → ReLU → MaxPool
Conv2D (32→64), kernel=3, padding=1 → ReLU → MaxPool
Conv2D (64→128), kernel=3, padding=1 → ReLU → MaxPool
Flatten (გამობრტყელება)
Fully Connected (128×6×6 → 256) → ReLU → Dropout(0.5)
Fully Connected (256 → 7) → გამოსავალი (logits)

გამოყენებული პარამეტრები:
ოპტიმიზატორი: SGD (lr=0.01, momentum=0.9)
Batch size: 64
Epoch-ები: 20
Learning rate scheduler: არ გამოყენებულა

ნორმალიზაცია: Pixel values / 255
Dataset: FER2013 (icml_face_data.csv) — გამოყენებულია Training და PrivateTest ნაწილები
Weight decay ან სხვა რეგულარიზაცია: არ გამოყენებულა

ცვლილებები პირველი მოდელიდან:
დამატებულია მესამე Convolution ფენა (Conv3) – რაც საშუალებას აძლევს მოდელს ისწავლოს უფრო ღრმა ვიზუალური მახასიათებლები.
Fully connected ფენა გაფართოვდა – ახლა აქვს 256 ნეირონი FC1-ში (პირველ ვერსიაში იყო 128).
ოპტიმიზატორი შეიცვალა – Adam-დან SGD-ზე გადადით, რაც ხშირად საჭიროებს მეტ Epoch-ს, თუმცა იძლევა უკეთეს გენერალიზაციას.
Epoch-ების რაოდენობა გაზრდილია 15-დან 20-მდე – ადაპტირებული ოპტიმიზატორის შეცვლასთან.

შედეგები და ანალიზი:
მეორე მოდელში შევნიშნე აშკარა გაუმჯობესება პირველთან შედარებით. Train Accuracy გაიზარდა 58%-დან 78%-მდე, ხოლო Validation Accuracy გაიზარდა 53%-დან 58%-მდე. ეს მიუთითებს, რომ მოდელმა უკეთ ისწავლა მონაცემებიდან, განსაკუთრებით მესამე Convolution ფენის და SGD ოპტიმიზატორის დამსახურებით.
მიუხედავად იმისა, რომ Train Accuracy საკმაოდ მაღალია, Validation Accuracy შედარებით ჩამორჩება და ბოლო Epoch-ებში შეინიშნება overfitting-ის ნიშნები – რაც ბუნებრივია უფრო ღრმა მოდელებისთვის.
Val Accuracy მერყეობდა 42%-დან 58%-მდე, ხოლო საუკეთესო შედეგი მივიღე Epoch 20-ზე – Val Accuracy: 58.5%. მთლიანობაში, ეს მოდელი ბევრად უკეთ მუშაობს, ვიდრე პირველი მარტივი CNN.




მოდელი 3 — CNN_BN_Dropout_Adam.ipynb

ცვლილებები წინა მოდელთან შედარებით (SimpleCNN-v2_Deeper_SGD)
მესამე მოდელში შევიტანე რამდენიმე მნიშვნელოვანი გაუმჯობესება, რომლებიც მიზნად ისახავდა მოდელის საერთო შედეგის (performance) გაუმჯობესებას:

Batch Normalization:
ყველა convolution ფენის შემდეგ დაემატა BatchNorm2d, რაც ხელს უწყობს სტაბილურ და სწრაფ სწავლას. ეს ამცირებს შიდა კოორდინაციების ცვლადობას (internal covariate shift) და აუმჯობესებს კონვერგენციას.
უფრო ღრმა სრულად დაკავშირებული ფენა (FC Layer):
Hidden ფენის ზომა 256-დან გაზრდილია 512-მდე — ეს ზრდის მოდელის შესაძლებლობას ისწავლოს უფრო რთული შაბლონები.
ოპტიმიზატორი - Adam:
Momentum-იანი SGD-ის ნაცვლად გამოყენებულია Adam ოპტიმიზატორი, რომელიც ავტომატურად ადაპტირებს სწავლის ტემპს თითოეული წონისთვის. ეს ხშირად იძლევა უკეთეს შედეგებს და უფრო სწრაფ კონვერგენციას.
დროის გაზრდა:
epochs-ის რაოდენობა 20-დან გაიზარდა 30-მდე, რაც მეტ საშუალებას აძლევს მოდელს ისწავლოს მონაცემებზე.
მიუხედავად იმისა, რომ არქიტექტურული და ოპტიმიზაციის მხრივ შეიტანეთ მნიშვნელოვანი გაუმჯობესებები, საბოლოო სატესტო შედეგები მკვეთრად არ გაუმჯობესებულა წინა მოდელთან შედარებით.
სწავლა უფრო ეფექტურად წარიმართა — ტრენინგის accuracy მუდმივად იზრდებოდა, ბოლოს მიაღწია 0.77-ს.





მოდელი 4 — FinaldFaceCNN
მოდელი FinaldFaceCNN წარმოადგენს არსებითად გაუმჯობესებულ კონვოლუციურ ნეირონულ არქიტექტურას, რომელიც შემუშავებულია FER2013 მონაცემთა ნაკრებზე Facial Expression Recognition ამოცანის გადასაჭრელად. მისი შექმნისას გამოყენებულია თანამედროვე deep learning ტექნიკები, როგორიცაა Batch Normalization, Skip Connection, Adaptive Pooling, და უფრო ღრმა, ფენებზე დაშენებული fully connected ბლოკი, რაც მნიშვნელოვნად აძლიერებს მოდელის გადამწყვეტუნარიანობას.

არქიტექტურული დეტალები:

Convolution ბლოკები:
Conv2D (1 → 64) + BatchNorm2d + LeakyReLU → MaxPooling
Conv2D (64 → 128) + BatchNorm2d + LeakyReLU → MaxPooling
Conv2D (128 → 256) + BatchNorm2d + LeakyReLU → MaxPooling
Conv2D (256 → 512) + BatchNorm2d + LeakyReLU
Skip Connection (Residual სტრუქტურა):
დამატებულია skip ბლოკი, რომელიც აწარმოებს პირდაპირ კავშირს x1 ფენიდან (Conv1-ის აუთპუტი) მეოთხე convolution ბლოკთან, რაც ხელს უწყობს გრადიენტის უკეთ გადაცემას და მოდელის სტაბილურობას.
Pooling:
ყველა convolution ბლოკს მოჰყვება MaxPooling ფენა.
ბოლოს გამოყენებულია AdaptiveAvgPool2d(1), რომელიც იძლევა უფრო მოქნილ და სკალირებად არქიტექტურას ნებისმიერი input ზომისთვის.
Fully Connected ბლოკი:
Linear(512 → 512) → BatchNorm1d → ReLU → Dropout(0.5)
Linear(512 → 256) → BatchNorm1d → ReLU → Dropout(0.3)
Linear(256 → 7) → Softmax (CrossEntropyLoss-ში ინახება)

ჰიპერპარამეტრები:
ოპტიმიზატორი: Adam
Learning rate: 0.001
Batch size: 64
Weight decay: 1e-5 (L2 რეგულარიზაცია)
Epoch-ების რაოდენობა: 50
Scheduler: ReduceLROnPlateau – ავტომატურად ამცირებს learning rate-ს ვალიდაციის სიზუსტის ზრდის შეჩერებისას

Dataset და Preprocessing:
გამოყენებულია FER2013 მონაცემთა ნაკრები (icml_face_data.csv)
სურათები გადაყვანილია grayscale-დან Tensor ფორმატში ToTensor საშუალებით
გამოყენებულია Normalize(mean=[0.5], std=[0.5])
Data Augmentation (მხოლოდ ტრენინგზე):
RandomHorizontalFlip()
RandomRotation(10)
WandB ინტეგრაცია:

მოდელის სწავლის ყველა მეტრიკა ლოგირდება Weights & Biases პლატფორმაზე (W&B), მათ შორის train_loss, val_loss, train_acc, val_acc, learning_rate, და ბოლოს test_accuracy.

მიზნები და გაუმჯობესებები წინა მოდელებთან შედარებით:

ღრმა არქიტექტურა და მეტი პარამეტრები: ახალი მოდელი შეიცავს ოთხი კონვოლუციური ფენის სტეკს და მნიშვნელოვან fully connected ბლოკს, რაც ზრდის გამომხატველობით შესაძლებლობას.
Batch Normalization: ყველა Conv და FC ფენაზე უზრუნველყოფს უფრო სტაბილურ და სწრაფ კონვერგენციას.
Dropout ფენები: ხელს უშლის overfitting-ს, განსაკუთრებით ღრმა მოდელების შემთხვევაში.
Skip Connection: აუმჯობესებს გრადიენტის გავრცელებას და ეხმარება მოდელს დასწავლის პროცესში.
Adaptive Pooling: იძლევა input-ზომებისადმი არამგრძნობიარე არქიტექტურას.
სწავლის ტემპის მართვა: Scheduler იძლევა მოდელის მოქნილ ადაპტაციას სწავლების პროცესში.

შედეგები და ანალიზი:
 სწავლება სტაბილურად პროგრესირებდა — საწყის ეტაპზე მოდელის სწორი კლასიფიკაციის მაჩვენებელი იყო 34.80% ტრენინგ სეტზე და 45.08% ვალიდაციაზე, ხოლო საბოლოოდ მივაღწიეთ 78.67%-იან სიზუსტეს ტრენინგზე და 65.23%-იან სიზუსტეს ვალიდაციაზე.

სწავლების განმავლობაში შეინიშნებოდა ერთგვაროვანი გაუმჯობესება როგორც დანაკარგში (loss), ისე სიზუსტეში (accuracy). საუკეთესო ვალიდაციის შედეგი დაფიქსირდა 27-ე ეპოქაზე, როცა Validation Accuracy მიაღწია 65.23%-ს და Validation Loss იყო 1.0344. სწორედ ეს წერტილი იქნა არჩეული როგორც საუკეთესო მოდელი.

დამატებით, საუკეთესო მოდელის საფუძველზე ჩატარდა შეფასება ტესტ სეტზე, რის შედეგადაც მიღებულ იქნა შემდეგი მაჩვენებლები:

Test Accuracy: 65.25%
Test Loss: 1.1044
ეს შედეგები მიუთითებს, რომ მოდელს გააჩნია კარგი გენერალიზაციის უნარი და ის ეფექტურად ასრულებს ემოციის კლასიფიკაციას მანამდე უხილავ მონაცემებზე.


